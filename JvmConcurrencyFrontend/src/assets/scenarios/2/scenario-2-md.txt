A thread is an operating system resource and is short for "thread of execution". A thread contains
the data, code, and any other resources needed for the CPU to execute a set of instructions. Modern day computers
have multiple processors, which means each CPU can execute a different thread at the same time. But the number of threads
running in parallel is not limited to the number of cores. How is this possible?

Well our computers can give the illusion of running threads in parallel on the same CPU by executing some of the code
in a thread and then swapping out another thread and executing some of its instructions and then swapping back. It does this so fast that
it gives the appearance of running in parallel, which is quite amazing. How it decides to switch threads
is very much platform dependent and a topic for another discussion, but suffice it to say that we have traditionally
had very little control over this.

So taking out example from scenario 1, let execute each of our http calls in its own thread to run them in parallel.

In the JVM we are given an `ExecutorService` interface that provides a layer of abstraction on creating and managing OS level threads.
A simple version is shown below. 

(If you aren't familiar with the annotations like `@Configuration` or `@Bean`, these are from [spring](https://spring.io/), a robust and battle tested
framework for backend application servers. It's beyond the scope of this topic, but would encourage anyone using the JVM for
backend development to give it a shot.) 

```
@Configuration
class UnboundThreadPoolExecutor {
    @Bean("unboundPoolExecutor")
    fun unboundPoolExecutor(): ExecutorService {
        return Executors.newCachedThreadPool()
    }
}
```

Using this ExecutorService lets change our code from before.

```
fun javaNetworkCallsExecutor(token: String): String {
    ThreadLogger.info("Service.javaNetworkCallsExecutor", "start")
    val context = MDC.getCopyOfContextMap()
    val future1 = unboundExecutorService.submit {
        MDC.setContextMap(context)
        javaHttpNetworkCall.execute(1400, token)
    }
    val future2 = unboundExecutorService.submit {
        MDC.setContextMap(context)
        javaHttpNetworkCall.execute(500, token)
    }
    future1.get()
    future2.get()
    ThreadLogger.info("Service.javaNetworkCallsExecutor", "end")
    return "success"
}
```

Now we are starting each http call in its own thread. Looking at the timeline we now see 3 different threads (the large blue bars our
log messages are sitting on). Select each one to see more information about what it was responsible for executing.

The first thread is the one that is handling the API request to our server. But we can see that the logs for
the http calls are now each in their own thread. We can also see the the "start" logs for each http call is happening at the same time. The first one still finishes after 1400ms, but our second one which only takes 700ms from the start.
Our total time is no longer these numbers added together, but instead is only as long as the slowest call.

That's it were done, right? Time to celebrate how great it is that we have concurred concurrency!

Not so fast!

The next [scenario](/unbound-thread-pool) reveals some potential issues.

